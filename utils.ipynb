{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ed11d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from n2v.models import N2VConfig, N2V\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from n2v.internals.N2V_DataGenerator import N2V_DataGenerator\n",
    "from scipy import sparse\n",
    "import os\n",
    "from tifffile import imwrite, imread\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "    \n",
    "def memory_alloc(n_go):\n",
    "    \"\"\"\n",
    "    Allocate memory on the GPU. If not activated the code will by default fill the GPU, without any performance gain.\n",
    "    \"\"\"\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "                [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024 * n_go)])\n",
    "        except RuntimeError as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20462f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_2d(data):\n",
    "    \"\"\"Reshape a 3d of 4d matrix into a 2d matrix.\"\"\"\n",
    "    mz = data.shape[-1]\n",
    "    n_pix = int(data.size/mz)\n",
    "    data = data.reshape(n_pix, mz)\n",
    "    return data\n",
    "\n",
    "def tif_to_matrix(dir_tif, extension = 'npz'):\n",
    "    \"\"\"Loads a dataset saved as a tif or tiff files in dir_tif and transforms it to a npy or npz matrix.\n",
    "    \"\"\"\n",
    "    list_im = os.listdir(dir_tif)\n",
    "    list_im = np.sort(list_im)\n",
    "    data = [imread(os.path.join(dir_scores_dn, f_name)) for f_name in list_im if '.tif' in f_name]\n",
    "    data = np.array(data)\n",
    "    data = np.moveaxis(data, 0, -1)\n",
    "    if extension = 'npz':\n",
    "        data = sparse.csr_matrix(reshape_2d(data))\n",
    "    return data\n",
    "\n",
    "def npz_to_tif(data_npz, shape, dir_out):\n",
    "    \"\"\"Saves the elements from a npz matrix into an output directory (dir_out) as a tiff file.\n",
    "    The last dimension of the matrix is assumed to be the channel axis.\"\"\"\n",
    "    data = sparse.csr_matrix.todense(data_npz)\n",
    "    data = np.array(data).reshape(shape)\n",
    "    data = np.moveaxis(data, -1,0)\n",
    "    data = list(data)\n",
    "    for i, im in enumerate(data):\n",
    "        imwrite(os.path.join(dir_out, f'{i:04}.tiff'), im)\n",
    "    \n",
    "\n",
    "def iontof_to_matrix(dir_csv, dir_out, d_type = '2d', extension = 'npz'):\n",
    "    \"\"\"Takes as input a file where IonTof data has been saved as txt file.\n",
    "    Returns and saves the data in the file as a npy or npz file.\"\"\"\n",
    "    list_csv = os.listdir(dir_csv)\n",
    "    data = np.zeros(shape = )\n",
    "    for f_name in list_csv:\n",
    "        data_full = []\n",
    "        if txt in f_name:\n",
    "            data_mz, shape = txt_tomatrix(f_fname, d_type)\n",
    "            data_full.append(data_mz)\n",
    "    mz = len(data_full)\n",
    "    data_full = np.array(data_full)\n",
    "    data_full = np.moveaxis(data_full, 0, -1)\n",
    "    if extension == 'npz':\n",
    "        sparse.save_npz(os.path.join(dir_out, 'data.npz'), sparse.csr_matrix(data_full))\n",
    "        elif extension == 'npy':\n",
    "            shape = shape + (mz,)\n",
    "            data = data.reshape(shape)\n",
    "            np.save(os.path.join(dir_out, 'data.npy'), data_full)\n",
    "    return data_full\n",
    "    \n",
    "def txt_to_matrix(f_csv, d_type = '2d'):\n",
    "    \"\"\"\n",
    "    Transforms a txt file exported from SurfaceLab into a matrix.\"\"\"\n",
    "    if d_type == '2d':\n",
    "        names = ['x', 'y', 'i']\n",
    "        dtype={'x': int, 'y': int, 'i': float}\n",
    "        elif d_type == '3d':\n",
    "            names = ['x', 'y', 'z', 'i']\n",
    "            dtype={'x': int, 'y': int, 'z': int, 'i': float}\n",
    "            else:\n",
    "                print('Data type not supported')\n",
    "\n",
    "    data_mz = pd.read_csv(os.path.join(dir_data, f_name),\n",
    "                          skiprows=10,\n",
    "                          delim_whitespace = True,\n",
    "                          header = None,\n",
    "                          names = names,\n",
    "                          dtype= dtype)\n",
    "        \n",
    "    x = data_mz['x'].to_numpy()\n",
    "    y = data_mz['y'].to_numpy()\n",
    "    i = data_mz['i'].to_numpy().astype(int)\n",
    "    \n",
    "    if d_type == '2d':\n",
    "        shape = (len(x), len(y))\n",
    "        data_temp = np.zeros(shape = shape)\n",
    "        data_temp[x, y] = i\n",
    "        elif d_type == '3d':\n",
    "            z = data_mz['z'].to_numpy()\n",
    "            shape = (len(x), len(y), len(z)))\n",
    "            data_temp = np.zeros(shape = shape)\n",
    "            data_temp[x, y, z] = i\n",
    "            \n",
    "    data_temp = data_temp.flatten()\n",
    "    sparse.save_npz(os.path.join(dir_save, f'{f_name}.npy'), sparse.csr_matrix(data_temp))\n",
    "    return data_temp, shape\n",
    "    \n",
    "            \n",
    "def load_data(f_name):\n",
    "    \"\"\"\n",
    "    Loads a MSI dataset saved as a .npz sparse matrix.\n",
    "    \"\"\"\n",
    "    data = sparse.load_npz(f_name)\n",
    "    data = np.array(sparse.csr_matrix.todense(data))\n",
    "    return data\n",
    "\n",
    "def compute_scores(data, f_dir, d_name, save = True):\n",
    "    print('Computing scores...')\n",
    "    \n",
    "    os.makedirs(f_dir, exist_ok = True)   \n",
    "    data = reshape_2d(data)\n",
    "    \n",
    "    # We now scale our data and compute the scores.\n",
    "    scale = StandardScaler()\n",
    "    data = scale.fit_transform(data)\n",
    "    pca = PCA()\n",
    "    scores = pca.fit_transform(data)\n",
    "    if save:\n",
    "        np.save(os.path.join(f_dir, f'{d_name}_scores.npy'), scores.astype('float16'))\n",
    "        np.save(os.path.join(f_dir, f'{d_name}_components.npy'), pca.components_.astype('float16'))\n",
    "    # We reshape the scores.    \n",
    "    return scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692c1ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def denoise_scores(j, scores_j,  batch_size, basedir, model_name, patch_shape = None,\n",
    "                  train_epochs = 100, split_val = 0.9):\n",
    "    \n",
    "    print(f'Denoising score {j}...')\n",
    "    datagen = N2V_DataGenerator()\n",
    "    dir_model = os.path.join(basedir, f'{model_name}')\n",
    "    dir_history = os.path.join(dir_model, 'history')\n",
    "    dir_denoised = os.path.join(dir_model, 'denoised_scores')\n",
    "    os.makedirs(dir_model, exist_ok = True)\n",
    "    os.makedirs(dir_denoised, exist_ok = True)\n",
    "    os.makedirs(dir_history, exist_ok = True)\n",
    "    \n",
    "    dat_j = data_augment(scores_j)\n",
    "    \n",
    "    dat_j = [np.expand_dims(dat, 0) for dat in dat_j]\n",
    "    dat_j = [np.expand_dims(dat, -1) for dat in dat_j]\n",
    "    \n",
    "    if patch_shape == None:\n",
    "        if scores_j.ndim ==2:\n",
    "            patch_shape = (64,64)\n",
    "            axes = 'YX'\n",
    "        if scores_j.ndim ==3:\n",
    "            patch_shape = (16,16,16)\n",
    "            axes = 'ZYX'\n",
    "        \n",
    "    # Generate patches\n",
    "    patches = datagen.generate_patches_from_list(dat_j, shape = patch_shape)\n",
    "    \n",
    "    np.random.shuffle(patches)\n",
    "    patches = patches.astype('float32') #float16 generates errors\n",
    "    train_break = int(patches.shape[0] * split_val)\n",
    "    X = patches[:train_break]\n",
    "    X_val = patches[train_break:]\n",
    "    \n",
    "    # Configuration of N2V\n",
    "    config = N2VConfig(X, unet_kern_size=3,\n",
    "                   train_steps_per_epoch = max(int(X.shape[0] / batch_size),1),\n",
    "                   train_epochs = train_epochs,\n",
    "                   train_loss='mse', batch_norm=True,\n",
    "                   train_batch_size = batch_size, n2v_perc_pix=0.198,\n",
    "                   n2v_patch_shape = patch_shape, \n",
    "                   n2v_manipulator = 'uniform_withCP',\n",
    "                   n2v_neighborhood_radius=5, single_net_per_channel=False)\n",
    "        \n",
    "    # We are now creating our network model.\n",
    "    model = N2V(config, model_name, basedir=basedir)\n",
    "\n",
    "    # Start training. The model saves itself automatically.\n",
    "    history = model.train(X, X_val)\n",
    "        \n",
    "    # Save history and denoised images\n",
    "    history = pd.DataFrame.from_dict(history.history)\n",
    "    history.to_csv(os.path.join(dir_history, f'history_{j:04}.csv'))\n",
    "    \n",
    "    if len(patch_shape)==2: axes= 'YX'\n",
    "    elif len(patch_shape)==3: axes= 'ZYX'\n",
    "    \n",
    "    score_dn = model.predict(scores_j, axes)\n",
    "    imwrite(os.path.join(dir_denoised, f'score_{j:04}.tiff'), score_dn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e336d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(data, dir_scores_dn, f_components):\n",
    "    \n",
    "    \"\"\"Reconstruct the dataset into a 2d matrix of dimensions (n_pix, n_mz)\"\"\"\n",
    "    \n",
    "    print('Reconstructing data...')\n",
    "    \n",
    "    data = data.astype('uint32')\n",
    "    data = reshape_2d(data)\n",
    "    means = np.mean(data, axis = 1)\n",
    "    std = np.std(data, axis = 1)\n",
    "\n",
    "    # Load components and scores\n",
    "    components = np.load(f_components)\n",
    "    scores_dn = tif_to_matrix(dir_scores_dn)\n",
    "\n",
    "    scores_dn = np.moveaxis(np.array(scores_dn), 0,2)\n",
    "    scores_dn = reshape_2d(scores_dn)\n",
    "    \n",
    "    # Reconstruct denoised data\n",
    "    data_dn = scores_dn @ components\n",
    "    data_dn = (data_dn.T * std + means).T\n",
    "    data_dn[data_dn<0]=0\n",
    "    \n",
    "    return data_dn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698f2fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def data_augment(data):\n",
    "    # This function augments the data by performing flips and transpositions.\n",
    "    print('Augmenting data...')\n",
    "    if data.ndim == 2: #data times 8\n",
    "        res = [np.rot90(data, k=i) for i in range(4)]\n",
    "        for dat in [np.rot90(data, k=i) for i in range(4)]:\n",
    "            res.append(np.flip(dat, 0))\n",
    "    if data.ndim == 3:\n",
    "        res = [np.rot90(data, k=i, axes = (0,1)) for i in range(4)] +[np.rot90(data, k=i, axes = (0,2)) for i in range(4)]\n",
    "        res2 = []\n",
    "        for dat in res:\n",
    "            res2 = res2 + [np.flip(dat, 0)]\n",
    "        res = res + res2\n",
    "        res2 = [np.rot90(data, k=i, axes = (1,2)) for i in range(4)]\n",
    "        for dat in res2:\n",
    "            res = res + [np.fliplr(dat)]\n",
    "        res = res + res2\n",
    "    print(f'Data was augmented by a factor {len(res)}.')\n",
    "    return res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
